{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI (XAI) with LIME and SHAP\n",
    "\n",
    "**Copyright (c) 2026 Shrikara Kaudambady**\n",
    "\n",
    "This notebook demonstrates how to use LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) to interpret the predictions of a machine learning model. We will train a simple classifier on the Iris dataset and then use these XAI techniques to understand its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, we import the necessary libraries and load the Iris dataset, which is a classic dataset for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Target names: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "We will train a `RandomForestClassifier`. The choice of model is not critical, as both LIME and SHAP are model-agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explainability with LIME\n",
    "\n",
    "LIME explains a single prediction by creating a local, interpretable model (like a linear model) around it. It perturbs the input and sees how the predictions change to infer feature importance.\n",
    "\n",
    "We will pick a single instance from the test set and see how LIME explains the model's prediction for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train, \n",
    "    feature_names=iris.feature_names, \n",
    "    class_names=iris.target_names, \n",
    "    mode='classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an instance to explain\n",
    "instance_idx = 10\n",
    "instance = X_test[instance_idx]\n",
    "true_label = y_test[instance_idx]\n",
    "\n",
    "# Generate the explanation\n",
    "explanation = explainer.explain_instance(\n",
    "    data_row=instance, \n",
    "    predict_fn=model.predict_proba, \n",
    "    num_features=len(iris.feature_names)\n",
    ")\n",
    "\n",
    "print(f\"Instance Details:\")\n",
    "for i, feature in enumerate(iris.feature_names):\n",
    "    print(f\"  {feature}: {instance[i]}\")\n",
    "\n",
    "print(f\"\nPrediction: {iris.target_names[model.predict(instance.reshape(1, -1))[0]]}\")\n",
    "print(f\"True Label: {iris.target_names[true_label]}\")\n",
    "\n",
    "# Visualize the explanation\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explainability with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) uses a game theory approach to explain model output. It calculates Shapley values, which represent the contribution of each feature to a prediction.\n",
    "\n",
    "SHAP can provide both global explanations (understanding the model as a whole) and local explanations (understanding a single prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SHAP explainer\n",
    "shap_explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = shap_explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Explanations with SHAP\n",
    "\n",
    "A SHAP summary plot gives a high-level overview of which features are most important for the model's predictions across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test, class_names=iris.target_names, feature_names=iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Explanation with SHAP\n",
    "\n",
    "A SHAP force plot shows how features contributed to push the model's prediction away from the base value. It's a powerful way to visualize the reasoning for a single prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize javascript for SHAP plots\n",
    "shap.initjs()\n",
    "\n",
    "# Explain the same instance as with LIME\n",
    "instance_idx_shap = 10\n",
    "\n",
    "# Create a force plot for one prediction\n",
    "# We look at the explanation for the predicted class\n",
    "predicted_class = model.predict(X_test[instance_idx_shap].reshape(1, -1))[0]\n",
    "\n",
    "shap.force_plot(\n",
    "    base_value=shap_explainer.expected_value[predicted_class],\n",
    "    shap_values=shap_values[predicted_class][instance_idx_shap, :],\n",
    "    features=X_test[instance_idx_shap, :],\n",
    "    feature_names=iris.feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook provided a brief introduction to Explainable AI using LIME and SHAP.\n",
    "\n",
    "- **LIME** is excellent for explaining individual predictions in a simple, intuitive way.\n",
    "- **SHAP** provides mathematically rigorous explanations, offering both powerful global insights (summary plots) and detailed local explanations (force plots).\n",
    "\n",
    "Incorporating these tools into the machine learning workflow is crucial for building trust, debugging models, and ensuring fairness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
